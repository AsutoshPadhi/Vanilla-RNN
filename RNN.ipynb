{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported packages successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Imported packages successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, hidden_size, vocab_size, h_init):\n",
    "        \n",
    "        ''' Hidden State '''\n",
    "        self.h = h_init\n",
    "        \n",
    "        ''' Input Weights '''\n",
    "        self.W_xh = np.random.randn(hidden_size, vocab_size)*0.01\n",
    "        \n",
    "        ''' Weights from previous state '''\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "        \n",
    "        ''' Output Weights '''\n",
    "        self.W_hy = np.random.randn(vocab_size, hidden_size)*0.01\n",
    "        \n",
    "        ''' Hidden Bias '''\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        ''' Output Bias '''\n",
    "        self.b_y = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        ''' Gradient Parameters '''\n",
    "        self.d_h_next = np.zeros_like(self.h)\n",
    "        self.d_W_xh = np.zeros_like(self.W_xh)\n",
    "        self.d_W_hh = np.zeros_like(self.W_hh)\n",
    "        self.d_W_hy = np.zeros_like(self.W_hy)\n",
    "        self.d_b_h = np.zeros_like(self.b_h)\n",
    "        self.d_b_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ''' Update hidden state '''\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x) + self.b_h)\n",
    "        \n",
    "        ''' Compute output vector '''\n",
    "        y = np.dot(self.W_hy, self.h) + self.b_y\n",
    "        \n",
    "        return self.h, y\n",
    "    \n",
    "    \n",
    "    def backward(self, p, h, h_prev, x, target):\n",
    "        \n",
    "        d_y = np.copy(p)\n",
    "        d_y[target] -= 1\n",
    "        \n",
    "        self.d_W_hy += np.dot(d_y, h.T)\n",
    "        \n",
    "        self.d_b_y += d_y\n",
    "        \n",
    "        ''' Backpropogating into h '''\n",
    "        d_h = np.dot(self.W_hy.T, d_y) + self.d_h_next\n",
    "        \n",
    "        ''' Backpropogating through tanh non linearity '''\n",
    "        d_h_raw = (1 - h*h)*d_h\n",
    "        \n",
    "        self.d_b_h += d_h_raw\n",
    "        \n",
    "        self.d_W_xh += np.dot(d_h_raw, x.T)\n",
    "        \n",
    "        self.d_W_hh += np.dot(d_h_raw, h_prev.T)\n",
    "        \n",
    "        self.d_h_next = np.dot(self.W_hh.T, d_h_raw)\n",
    "        \n",
    "        return self.d_W_hy, self.d_W_hh, self.d_W_xh, self.d_b_h, self.d_b_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size = 1115394 , Vocabulary size = 65\n"
     ]
    }
   ],
   "source": [
    "''' Read the text file '''\n",
    "data = open(\"shakespere.txt\", \"r\").read()\n",
    "\n",
    "''' chars is a set of all characters in the text file '''\n",
    "chars = list(set(data))\n",
    "\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(\"Data size = \"+str(data_size)+\" , Vocabulary size = \"+str(vocab_size))\n",
    "\n",
    "''' Indexing every character '''\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Size of hidden layers of neurons '''\n",
    "hidden_size = 100\n",
    "\n",
    "''' Number of steps to roll the RNN for '''\n",
    "seq_length = 25\n",
    "\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    inputs is a list of integers containing the indices of characters from input.txt\n",
    "    targets is a list of integers containing the indices of the next character from input.txt\n",
    "    h_init is the the initial hidden state\n",
    "'''\n",
    "def lossFunction(inputs, targets, h_init):\n",
    "    \n",
    "    ''' These are sets containing values at each time period '''\n",
    "    '''\n",
    "        x_set : one-hot input at each t\n",
    "        h_set : hidden state at each t\n",
    "        y_set : output at each t (set of probabilities for the next character)\n",
    "        p_set : set of probabilities at each t (normalized)\n",
    "    '''\n",
    "    x_set, h_set, y_set, p_set = {}, {}, {}, {}\n",
    "    \n",
    "    h_set[-1] = np.copy(h_init)\n",
    "    loss = 0\n",
    "    \n",
    "    \n",
    "    ''' --------------------------------- Forward Pass --------------------------------- '''\n",
    "    \n",
    "    ''' Iterate for every character '''\n",
    "    for t in range(len(inputs)):\n",
    "        \n",
    "        ''' One Hot encoding of the input '''\n",
    "        x_set[t] = np.zeros((vocab_size, 1))\n",
    "        x_set[t][inputs[t]] = 1\n",
    "        \n",
    "        ''' Forward propogation '''\n",
    "        h_set[t], y_set[t] = model.forward(x_set[t])\n",
    "        \n",
    "        ''' Normalize probability '''\n",
    "        p_set[t] = np.exp(y_set[t]) / np.sum(np.exp(y_set[t]))\n",
    "        \n",
    "        ''' Softmax loss '''\n",
    "        loss += -np.log(p_set[t][targets[t],0])\n",
    "    \n",
    "    \n",
    "    ''' --------------------------------- Backward Pass --------------------------------- '''\n",
    "    \n",
    "    ''' Iterate backwards '''\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        ''' Backpropogate '''\n",
    "        d_W_hy, d_W_hh, d_W_xh, d_b_h, d_b_y = model.backward(p_set[t], h_set[t], h_set[t-1], x_set[t], targets[t])\n",
    "        \n",
    "    ''' Clipping to mitigate exploding gradient '''\n",
    "    for d_param in [d_W_hy, d_W_hh, d_W_xh, d_b_h, d_b_y]:\n",
    "        np.clip(d_param, -5, 5, out = d_param)\n",
    "        \n",
    "    return loss, d_W_hy, d_W_hh, d_W_xh, d_b_h, d_b_y, h_set[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_index, n):\n",
    "    \n",
    "    ''' Encode as one-hot vector '''\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    x[seed_index] = 1\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    for t in range(n):\n",
    "        h, y = model.forward(x)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        idx = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        indices.append(idx)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k;fbZT:ITco'YXEeOjCLPGPe$3WfV:!R:EyR,nW,ApAznMpxY?oWQ!jVYxuJMuQAumRGeDs;SniW!ecZF'$jza,yD\n",
      "gJfQIG;ecGkFut:OFYWh&uOtfv-vIDqgtaje-erne3JvOm!hoH3\n",
      "J\n",
      "H!jEEoy'GcuGrZaPh?BLqX&ysQGKM3Hf-dMMA$QvwODzi&QLycv!,. :\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 0, loss: 104.35968020107893\n",
      " sr nhs,n$h\n",
      "sph   eon h\n",
      "lh w  ionhr\n",
      "nHei psWshw\n",
      " ps\n",
      " p onpione naplnuH o  z\n",
      "  l u:st slot:lsnhho :co hd\n",
      "\n",
      "rennpwonhlon:cnnhsnshzW el,  ht slWiHlW  hosh Wn  \n",
      "  honphWt:hon eW Ho\n",
      "sp\n",
      " n: \n",
      "nhh\n",
      " pcn neW :e \n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 100, loss: 112.43478320665187\n",
      "Clehrturiter ,en,sdn,skNsiuSmttna ubssphytirimtnatv dasc,terCto lupniei'ossnltphTtunrdin iinsseraitr,avnatpb,ytnTtlhT pr,tsn,itbston,iih eo t pnTiihltinT tcTtvnTsbriiinTaerrtohasonTttnCtintsteaophTtin\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 200, loss: 116.99325315502337\n",
      "r\n",
      "es neey\n",
      "os NTtt\n",
      "Ts ue  ussc\n",
      "T tn\n",
      "c g,cr \n",
      "t \n",
      "s  gol \n",
      "eashsL?\n",
      "kl \n",
      "Ttt\n",
      "ror\n",
      ", ugttt\n",
      "s  oWcrhbt g tw\n",
      "st bor rooy\n",
      "ol ni ths.w W tgsstuost\n",
      "ktwge  ho s\n",
      "W rNo  \n",
      "v  ga awsih\n",
      "t a\n",
      "\n",
      "l u\n",
      "s Nkath er\n",
      "sl rsLrnel?\n",
      "Tc\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 300, loss: 121.35902057233905\n",
      "Oyyktsoh on CsetssrnOowroseiCmeeadn Cswnesnimsnt oo ds p E b'swi sr  onalonkthneeswnto nts itinntln t\n",
      "uiedn-ssnpChn srn s,r towkassnosWnOynh , ta niotd sen  mo oi iCs\n",
      "uao t' ritoWrCsrborctnsedrtnd onk\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 400, loss: 123.49924793590604\n",
      "AIstiag!t\n",
      "NJAsanMs\n",
      "hmotr\n",
      "a aied! hs ie :ies! \n",
      "Nie nkiadkAuN bp !r\n",
      " s pcrroe sutnbn   oe!ifee stw ae  hortso rhsei\n",
      "un\n",
      "ookmp r I !too!AI\n",
      " eeo!As dtI \n",
      "T s AIhr :sa  oiehrwiueaAo stIeabpeJ\n",
      "a\n",
      "!AINnTe  ia n\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 500, loss: 127.0630390694452\n",
      "blen TS :h, blaBlhmn\n",
      "   n  oh \n",
      "es \n",
      "onni:elhBD Oeb ho\n",
      "lIrnUko Tydat\n",
      "ohu,:. tebh r r,r:S.obh B r.ob ooWnes n\n",
      "Bceeos ,nhrao o srl ab esc!es \n",
      "lnn w h gs\n",
      "Twl:,wiorFie eise.\n",
      "ceinbla cherT\n",
      "yobrmocTato OibtOs\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 600, loss: 127.18131604639323\n",
      "oa euaIbrrvnda,  dtn an\n",
      "o \n",
      " rtoGe tyrsUyIhtbema\n",
      "d,\n",
      "\n",
      "rhl  ste hnyeaol atb.ate-'hor vy ioy  \n",
      "ydht et \n",
      "eow rutd aebr \n",
      "nr\n",
      "tyh oyshtnu n ril  a\n",
      "aoAenrisnra\n",
      "er'oeeaonuat  rIyrotnratnlsn ra\n",
      "bran:u'tedttsrat:\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 700, loss: 128.9303985518272\n",
      " hds.haA trl adAhom\n",
      ".fI  f\n",
      " n\n",
      "Ip .\n",
      "shae c R \n",
      " Raouas femh  ,Ibelc\n",
      "\n",
      "l hrl.e\n",
      "ni \n",
      "s am,Ibmt renmbegIee\n",
      "e osSe: h:rlIo\n",
      "l.aee.\n",
      ":I.\n",
      "Ru.aelctu  eetcT:l oentdesno\n",
      "ghoest\n",
      "e cf:chbetSedl resuuwoIfRlceecoras org\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 800, loss: 130.65976002140832\n",
      "t\n",
      "f tueat\n",
      "jsas otect detusomloe hW oahniro;RwWehfpe wCeilWlAwd t:\n",
      "e tIyTrrscrdeRaueS,\n",
      "mSos caIt hht  oe hhclol lwoe r siahilh\n",
      "la:,nteCnrohtS w nrheR aetrinmhdMStWsS\n",
      " otohe wperl\n",
      "\n",
      "i m\n",
      "e   fleli\n",
      "vjAnhyo\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 900, loss: 130.68089055488912\n",
      "yp\n",
      "dS:ilythftei iNhw prty hltC m \n",
      "hbSeO\n",
      "to LthOkoshlttrat  aoselyNOttshlt.wSt\n",
      "\n",
      "sT \n",
      "l :Oto:hltphltohll ri rw t oSyptnlCwnACit Chsnpri  wfFueeeeht unlyuhfs rio eeiThi  hie ri phfspw\n",
      "SCatoSwonCe\n",
      "pN twucw\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 1000, loss: 130.39398957221826\n",
      "mdd  nra iea ney toeiatel'fh:fvheothr te itr,lel rrlnwde\n",
      "rs sbf oyr 'yh Ibtl fveI teoot sovehdee.ise ane oehadtenbs  bres  d,Fsh oreI'isnn eeioyoitrIbge nt  iy n\n",
      "etn' uIim t\n",
      "g:Ifde a edithtidelberdi t\n",
      "------------------------------------------------------------------------------------------------\n",
      "iter 1100, loss: 130.03321934426987\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0f4c58761ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;34m''' Training '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_W_hy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_W_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_W_xh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_b_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_b_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;34m''' Calculate Loss '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-61256c2e8ad6>\u001b[0m in \u001b[0;36mlossFunction\u001b[0;34m(inputs, targets, h_init)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34m''' Backpropogate '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0md_W_hy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_W_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_W_xh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_b_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_b_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m''' Clipping to mitigate exploding gradient '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-67cf0dcf46c8>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, p, h, h_prev, x, target)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_W_hh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_h_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_h_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_hh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_h_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_W_hy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_W_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_W_xh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_b_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_b_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' Iterative index '''\n",
    "n = 0\n",
    "\n",
    "''' Data pointer '''\n",
    "p = 0\n",
    "\n",
    "''' Memory variables for Adagrad '''\n",
    "m_W_xh = np.zeros((hidden_size, vocab_size))\n",
    "m_W_hh = np.zeros((hidden_size, hidden_size))\n",
    "m_W_hy = np.zeros((vocab_size, hidden_size))\n",
    "m_b_h = np.zeros((hidden_size, 1))\n",
    "m_b_y = np.zeros((vocab_size, 1))\n",
    "\n",
    "''' Loss at iteration 0 '''\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length\n",
    "\n",
    "''' Initialize the RNN class '''\n",
    "h_init = np.zeros((hidden_size,1))\n",
    "model = RNN(hidden_size, vocab_size, h_init)\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ''' Take input from left to right, seq_lenth at a time '''\n",
    "    if p + seq_length + 1 >= len(data) or n == 0: \n",
    "        ''' Reset RNN memory '''\n",
    "        h_init = np.zeros((hidden_size,1))\n",
    "        ''' Go from the start of the data '''\n",
    "        p = 0\n",
    "    \n",
    "    ''' Pick seq_length number of characters as inputs and targets '''\n",
    "    inputs = [char_to_idx[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_idx[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    \n",
    "    ''' Sample from the model '''\n",
    "#     print(\"\\n\")\n",
    "    if n%100 == 0:\n",
    "        sample_idx = sample(h_init, inputs[0], 200)\n",
    "        txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "        print(txt)\n",
    "        \n",
    "    \n",
    "    ''' Training '''\n",
    "    loss, d_W_hy, d_W_hh, d_W_xh, d_b_h, d_b_y, h_init = lossFunction(inputs, targets, h_init)\n",
    "    \n",
    "    ''' Calculate Loss '''\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    ''' Print status '''\n",
    "    if n % 100 == 0:\n",
    "        print(\"------------------------------------------------------------------------------------------------\")\n",
    "        print(\"iter \"+str(n)+\", loss: \"+str(smooth_loss))\n",
    "        \n",
    "    ''' Parameter updating using Adagrad '''\n",
    "    for param, dparam, mem in zip([model.W_xh, model.W_hh, model.W_hy, model.b_h, model.b_y], \n",
    "                                [d_W_xh, d_W_hh, d_W_hy, d_b_h, d_b_y],\n",
    "                                [m_W_xh, m_W_hh, m_W_hy, m_b_h, m_b_y]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "        \n",
    "    p += seq_length\n",
    "    n += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
